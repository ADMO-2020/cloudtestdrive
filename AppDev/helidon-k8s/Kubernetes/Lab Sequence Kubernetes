*** Demo script contents, make unusable (bad characters etc.) but use the scripts to do the work ***

It is assumed that you have done a docker login to the appropriate registry and have pushed the storefront and stockmanager
to the repository

FOR NOW it seems that k8s on a laptop can't correctly authenticate to the devcs repos if they are private, so make sure the repo is
set to public. If this is fixed need to have an imagePullSecret setup 

The above section needs more explanation, possible also tie it in to a devcs build pipeline ?

Kuberneties
===========
Docker is great, but it only runs on a local machine, and doesn't have all of the nice cloud native features of kuberntes.
Let's see how to get those working.

It is assumed you have access to a Kuberneties cluster via kubeconf.

Point to Jan's k8s cluster setup scripts ?

<Need description of k8s here ??? >

Basic cluster infrastructure elements install
=============================================
For most things in Kubernetes the Helm is used to install and configure not just the pods, but also the configuration around them.
Helm has templates (called charts) that define how to install potentially multiple services and to set them up.

Using Helm 3
Helm 3 is a client side only program that is used to configure the kubernetes cluster with the services you chose. 

Helm is installed in the virtual machine, but if you need to get it in the future there are details in the [Helm web page](https://helm.sh/docs/intro/install/)

Follow the instructions in the helm/Helm install file to download helm and install it. 

Once you have Helm then use it to install kuberneties-dashboard (if you didn't install it when you setup the cluster) There are instructions
in the Kuberneties-dashboard folder. Remember to run the kubeproxy, and to use the kubeconf commands to get the access token to log in with.

Ingress controller

Running your containers in Kuberneties
======================================

You now have the basic environment to deploy services.
There are scripts to cover most of these steps, if you're on a Unix machine they will work just fine, but let's go through the steps.

Namespaces
==========

Firstly if you are running on a shared cluster you need to create a namespace, this is basically a "virtual" cluster that let's us separate 
out services from others that may be running in the cluster. If you have your own cluster it's still a good idea to have your own namespace
so you can separate the lab from other activities in the cluster, letting you easily see what's happening in the lab and also no interfering 
with other cluster activities, and also easily delete it if needs be (deleting a namespace deletes everything in it)

Do not do the following, there is a script here, but the follwing do this
kubectl create namespace <my namespace name>


The create-namespace script sets up a namespace and also sets is as the default to use for other kubeconf operations (unless you override them) 
It will also delete any existing namespace with the same name. As well all have our own clusters we don't actually need to do this, but it's good practice, so let's create one for the helidon lab

create-namespace.sh helidon

kubectl get pods 
nothing showing
kubectl get pods -A
all namespaces

If using the dashbaord remember that you're using the namespace, not the default !

Secrets and external configuration
==================================

Once you have a namespace the next step is to create the secrets. Secrets are things like configurations or database logins. This information
could be provided to a cluster in many ways, but this is a nice simple one. A secret can be exposed to a pod and means that the information it 
contains does not have to be build in to the docker image, but can be 

The create-secrets.sh script sets up the secrets in the namespace you've chosen as your default

Look in the dashboard (remember to use the right namespace !)

Services
========

The next step is to create services, a service is basically a description of a set of connected microservices and the port(s) they listen on. 
These can be for different microservcies, different versions or the same version a microservice. A service effectively defines a logical endpoint that has a internal dns name inside the cluster 
to enable communication to a service. It's also internal load ballancer in that 
if there are multiple pods for a service it will switch between the pods, and also will remove pods from it's load ballancer if they are not 
operating properly.

Services determine what pods they will talk to using selectors. each pod had meta data comprised of multiple name / value pairs that can be searched
on (e.g. type=dev, type=test, type=production, app=stockmanager etc.) The service has a set of labels it will match on (within the namespace) and
gets the list of pods that match from kubernetes and uses that information to setup the dns and round robin load balancing

Services can be exposed externally via load balancer (the type) but by default are only visible inside the cluster.

*** INCLUDE EXAMPLE SERVICE DEFINITION YAML AND EXPLAIN IT ***

The servicesClusterIP.yaml will define the services we will be using here, and the script setupClusterIPServices.sh will apply that file creating 
the services

Important, you need to define the services before defining anything else (e.g. deployments, pods, ingress rules etc.) that may use them, this 
is especially true of pods which may look for dNS names that are dynamically created when services start up

Note that the service defines the endpoint, it's not actually running any code for your service yet.

walk through services in dashboard / kubectl, no podds associated with it

Accessing your services using an ingress
=========================================
Services can configure externally visible  load balancers for you, however this is not recommended for several reasons if using REST type 
accesses. 

Firstly the load balancers that are created are not part of kuberneties, the service needs to communicate with the external cloud
infrastructure to create them, this means that the cloud needs to provide load balancers and drivers for kubernetes to configure them, noit all
clouds may provide this in a consistent manner, so you may get unexpected behavior.

Secondly the load balancers are at the port level, this is fine if you are dealing with a TCP connection (say a JDBC driver) however it
means that you can't inspect the data contents and take actions based on it (for example requiring authentication)

Thirdly most cloud services charge on a per load balancer basis, this means that if you need to expose 10 different REST endpoints you are
paying for 10 separate load balancers

Fourthly from a security perspective it means that you can't do things like enforcing SSL on your connections, as that's done at a level above TCP/IP

Fortunately for REST activities there is another option, the ingress controller. This can service multiple REST API endpoints as it operates at the http
level and is aware of the context of the request (e.g. URL, headers etc.) The downside of an ingress controller is that it does not operate on non http 
requests

There are many different ingress controllers (and sadly they do not all have exactly the same configuration options) but for this lab we're going
to use the nginx based one.

Use helm to install the nginx ingress controller if it's not already there - see the Ingress front end setup document. This installs the 
nginx ingress controller (the actuall ingress processing engine) and also configures a single load balancer to allow external connections into the 
cluster (mulpile instances of an ingress controller are of course posisble for HA, but here we'll just use a single one) The controller itself
doesn't configure the actual ingress rules. 

Once we have the ingress controller service running service endpoints we can define the ingress rules that will apply. these rules define 
URL's and service endpoints to pass those URLs to, the URL's can also be re-written if desired.

*** INCLUDE COMMENTED EXAMPLE YAML ***

The file ingressConfig.yaml defines the ingress rules, the script setupIngress.sh will apply the config for us. This will configure the ingress controller
with the URL and service processing details.

We can interact with the deployment using the public side of the ingress (it's load balancer) if running a local cluster this will be 
localhost:80, but in a cloud deployment use the kubectl get services and see the public IP address of the ingress controllers load balancer,
or the dashboard. Of course as there are no services actually running yet the ingress will report errors if you do try to access it

*** NEED TO SHOW HOW TO GET THE LIST OF ENDPOINTS***

Need to explain the service + ingress but no pods

Deploying the actual microservices
==================================

We're finally ready to create the deployments. A deployment is the microservice itself, this can be one or more pods. The deployment itself handles
things like rolling upgrades by manipulating the replica sets. A replica set is a group of pods, it will ensure that if a pod fails that another will be started.
It's possible to add and remove replicas form a pod, but the key thing to note is that all pods within a replica are the same. Finally we have
the pods. In most cases a pod will contain a single user container (based on the image you supply) however it may also contain kubernetes
internal containers (for example to handle network redirections)

*** walk through example ***

The stockmanager-deployment.yaml, storefront-deployment.yaml and zipkin-deployment.yaml files contain the deployments, the script deploy.sh will
load them into the cluster.

kubectl get pods
kube-dashboard

look at services see there are pods for the service.


Look at one of the deployments <Deployment description here>

We now have our system running in the cluster, look at the dashboard to see what's happening or do kubectl get all
We can interact with the deployment using the public side of the ingress (it's load ballancer) if running a local cluster this will be localhost:80, but in a cloud
deployment use the kubectl get services and see the public IP address of the ingress controlers load ballancer, or the dashboard

e.g. to access zipkin go to web page http://<load ballancer ip>/zipkin
to make a request to the "core" use postman http://<load ballancer ip>/store
Or you can go to specific services http://<loadballancer ip>/smmgt/health (stock manager mmanagelemtn port)
Or you can go to specific services http://<loadballancer ip>/sf/minimumChange

do we want to look at logs in the dashboard when making a request 

Enhancing the capabilities
==========================
We've now basically depoloyed our microservcies into kuberneties, but there are many additional things that kuberneties can do for us

Replica sets and updating the deployment
========================================
Replica sets section

Monitoring and reporting
========================
Prometheus and grafana sections

Auto scaling
============
Metrics don't seem to work on the desktop k8s, need to work this out in a cloud version

Securing your service external endpoints
========================================
Using the ingress controller to enforce various capabilities
https://docs.bitnami.com/kubernetes/how-to/secure-kubernetes-services-with-ingress-tls-letsencrypt/


Managing the cluster network with a service mesh
=================================================

Canary deployments
==================
https://istio.io/blog/2017/0.1-canary/

A/B testing
===========
https://dzone.com/articles/ab-testing-with-kubernetes-and-istio
https://dzone.com/articles/ab-testing-with-kubernetes-and-istio